# Language_Recognizer
This is a basic deep neural network model that can recognize three sign language gestures:

âœ‹ "hello"

ğŸ™ "thanks"

ğŸ¤Ÿ "iloveu"

The system:

Works in real time using OpenCV for video capture

Uses Mediapipe to detect and show hand landmarks

Classifies detected gestures using a pre-trained model (action.h5)

Allows adding more signs easily by expanding the training dataset and model

ğŸ“‚ Repository Structure

Basic-Sign_Language-Recognizer/
â”œâ”€â”€ 0.npy                         # Preprocessed training data (landmarks/coords)
â”œâ”€â”€ README.md                     # Project description
â”œâ”€â”€ action.h5                     # Pre-trained deep learning model for gesture classification
â”œâ”€â”€ original sign recognition.ipynb # Jupyter Notebook with full implementation
âš™ï¸ How It Works
Capture Video â€“ OpenCV captures the live video feed.

Detect Hand Landmarks â€“ Mediapipe extracts key points of the hand.

Feature Processing â€“ The coordinates are normalized and converted into a feature vector.

Gesture Classification â€“ The pre-trained model (action.h5) predicts the gesture.

Real-time Display â€“ Recognized gesture is displayed on the video frame.
